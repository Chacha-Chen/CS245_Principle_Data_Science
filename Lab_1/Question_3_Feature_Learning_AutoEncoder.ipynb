{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Auto-Encoder to reduce the dimensionality of the features. Then evaluate the effectiveness via training a linear SVM with the new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and split it into training set (60%) and test set (40%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_file = 'AwA2-features/Animals_with_Attributes2/Features/ResNet101/AwA2-features.txt'\n",
    "labels_file = 'AwA2-features/Animals_with_Attributes2/Features/ResNet101/AwA2-labels.txt'\n",
    "\n",
    "# There is in total 37322 images of 50 classes. Each image is represented as a 2048 dimensional feature\n",
    "features = np.loadtxt(features_file) # shape (37322, 2048)\n",
    "labels = np.loadtxt(labels_file) # shape (37322, )\n",
    "\n",
    "# Split each and all classes into training set (60%) and test set (40%)\n",
    "# set random_state to an int for reproducibility\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    features, labels, train_size=0.6, test_size=0.4, random_state=0, stratify=labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design an auto-encoder architecture (with only fully connected layers) and train the neural network in the conventional supervised learning manner. The labels of the training set is, however, no longer the class categories but rather the input itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = X_train.shape[1]\n",
    "\n",
    "# Define the encoder dimension\n",
    "encoding_dim = 250\n",
    "\n",
    "layer0 = Input(shape=(d, ))\n",
    "\n",
    "# Encoder Layers\n",
    "encoding_layer1 = Dense(2000, activation='relu')(layer0)\n",
    "encoding_layer2 = Dense(1750, activation='relu')(encoding_layer1)\n",
    "encoding_layer3 = Dense(1500, activation='relu')(encoding_layer2)\n",
    "encoding_layer4 = Dense(1250, activation='relu')(encoding_layer3)\n",
    "encoding_layer5 = Dense(1000, activation='relu')(encoding_layer4)\n",
    "encoding_layer6 = Dense(750, activation='relu')(encoding_layer5)\n",
    "encoding_layer7 = Dense(500, activation='relu')(encoding_layer6)\n",
    "encoding_layer8 = Dense(encoding_dim, activation='relu')(encoding_layer7)\n",
    "\n",
    "# Decoder layers\n",
    "decoding_layer1 = Dense(500, activation='relu')(encoding_layer8)\n",
    "decoding_layer2 = Dense(750, activation='relu')(decoding_layer1)\n",
    "decoding_layer3 = Dense(1000, activation='relu')(decoding_layer2)\n",
    "decoding_layer4 = Dense(1250, activation='relu')(decoding_layer3)\n",
    "decoding_layer5 = Dense(1500, activation='relu')(decoding_layer4)\n",
    "decoding_layer6 = Dense(1750, activation='relu')(decoding_layer5)\n",
    "decoding_layer7 = Dense(2000, activation='relu')(decoding_layer6)\n",
    "decoding_layer8 = Dense(d, activation='relu')(decoding_layer7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20153 samples, validate on 2240 samples\n",
      "Epoch 1/60\n",
      "20153/20153 [==============================] - 1834s 91ms/step - loss: 0.5377 - val_loss: 0.5025\n",
      "Epoch 2/60\n",
      "20153/20153 [==============================] - 105s 5ms/step - loss: 0.4472 - val_loss: 0.4064\n",
      "Epoch 3/60\n",
      "20153/20153 [==============================] - 104s 5ms/step - loss: 0.3955 - val_loss: 0.3861\n",
      "Epoch 4/60\n",
      "20153/20153 [==============================] - 102s 5ms/step - loss: 0.3800 - val_loss: 0.3701\n",
      "Epoch 5/60\n",
      "20153/20153 [==============================] - 101s 5ms/step - loss: 0.3689 - val_loss: 0.3760\n",
      "Epoch 6/60\n",
      "20153/20153 [==============================] - 99s 5ms/step - loss: 0.3725 - val_loss: 0.3853\n",
      "Epoch 7/60\n",
      "20153/20153 [==============================] - 100s 5ms/step - loss: 0.3600 - val_loss: 0.3526\n",
      "Epoch 8/60\n",
      "20153/20153 [==============================] - 101s 5ms/step - loss: 0.3420 - val_loss: 0.3374\n",
      "Epoch 9/60\n",
      "20153/20153 [==============================] - 105s 5ms/step - loss: 0.3339 - val_loss: 0.3318\n",
      "Epoch 10/60\n",
      "20153/20153 [==============================] - 105s 5ms/step - loss: 0.3266 - val_loss: 0.3292\n",
      "Epoch 11/60\n",
      "20153/20153 [==============================] - 104s 5ms/step - loss: 0.3204 - val_loss: 0.3199\n",
      "Epoch 12/60\n",
      "20153/20153 [==============================] - 109s 5ms/step - loss: 0.3129 - val_loss: 0.3146\n",
      "Epoch 13/60\n",
      "20153/20153 [==============================] - 116s 6ms/step - loss: 0.3092 - val_loss: 0.3098\n",
      "Epoch 14/60\n",
      "20153/20153 [==============================] - 109s 5ms/step - loss: 0.3053 - val_loss: 0.3078\n",
      "Epoch 15/60\n",
      "20153/20153 [==============================] - 110s 5ms/step - loss: 0.3051 - val_loss: 0.3058\n",
      "Epoch 16/60\n",
      "20153/20153 [==============================] - 111s 5ms/step - loss: 0.3015 - val_loss: 0.3043\n",
      "Epoch 17/60\n",
      "20153/20153 [==============================] - 103s 5ms/step - loss: 0.2977 - val_loss: 0.3045\n",
      "Epoch 18/60\n",
      "20153/20153 [==============================] - 102s 5ms/step - loss: 0.2959 - val_loss: 0.3229\n",
      "Epoch 19/60\n",
      "20153/20153 [==============================] - 103s 5ms/step - loss: 0.2984 - val_loss: 0.3012\n",
      "Epoch 20/60\n",
      "20153/20153 [==============================] - 100s 5ms/step - loss: 0.2909 - val_loss: 0.2972\n",
      "Epoch 21/60\n",
      "20153/20153 [==============================] - 100s 5ms/step - loss: 0.2919 - val_loss: 0.2971\n",
      "Epoch 22/60\n",
      "20153/20153 [==============================] - 100s 5ms/step - loss: 0.2892 - val_loss: 0.2979\n",
      "Epoch 23/60\n",
      "20153/20153 [==============================] - 100s 5ms/step - loss: 0.2874 - val_loss: 0.2988\n",
      "Epoch 24/60\n",
      "20153/20153 [==============================] - 100s 5ms/step - loss: 0.2840 - val_loss: 0.2925\n",
      "Epoch 25/60\n",
      "20153/20153 [==============================] - 100s 5ms/step - loss: 0.2878 - val_loss: 0.3044\n",
      "Epoch 26/60\n",
      "20153/20153 [==============================] - 100s 5ms/step - loss: 0.2878 - val_loss: 0.2965\n",
      "Epoch 27/60\n",
      "20153/20153 [==============================] - 100s 5ms/step - loss: 0.2825 - val_loss: 0.2919\n",
      "Epoch 28/60\n",
      "20153/20153 [==============================] - 100s 5ms/step - loss: 0.2814 - val_loss: 0.2927\n",
      "Epoch 29/60\n",
      "20153/20153 [==============================] - 99s 5ms/step - loss: 0.2807 - val_loss: 0.2910\n",
      "Epoch 30/60\n",
      "20153/20153 [==============================] - 95s 5ms/step - loss: 0.2794 - val_loss: 0.2913\n",
      "Epoch 31/60\n",
      "20153/20153 [==============================] - 95s 5ms/step - loss: 0.2787 - val_loss: 0.2907\n",
      "Epoch 32/60\n",
      "20153/20153 [==============================] - 95s 5ms/step - loss: 0.2776 - val_loss: 0.2918\n",
      "Epoch 33/60\n",
      "20153/20153 [==============================] - 95s 5ms/step - loss: 0.2762 - val_loss: 0.2898\n",
      "Epoch 34/60\n",
      "20153/20153 [==============================] - 95s 5ms/step - loss: 0.2798 - val_loss: 0.2895\n",
      "Epoch 35/60\n",
      "20153/20153 [==============================] - 95s 5ms/step - loss: 0.2765 - val_loss: 0.2869\n",
      "Epoch 36/60\n",
      "20153/20153 [==============================] - 95s 5ms/step - loss: 0.2733 - val_loss: 0.2876\n",
      "Epoch 37/60\n",
      "20153/20153 [==============================] - 96s 5ms/step - loss: 0.2736 - val_loss: 0.2874\n",
      "Epoch 38/60\n",
      "20153/20153 [==============================] - 95s 5ms/step - loss: 0.2714 - val_loss: 0.2883\n",
      "Epoch 39/60\n",
      "20153/20153 [==============================] - 95s 5ms/step - loss: 0.2720 - val_loss: 0.2880\n",
      "Epoch 40/60\n",
      "20153/20153 [==============================] - 95s 5ms/step - loss: 0.2718 - val_loss: 0.2887\n",
      "Epoch 41/60\n",
      "20153/20153 [==============================] - 95s 5ms/step - loss: 0.2747 - val_loss: 0.2914\n",
      "Epoch 42/60\n",
      "20153/20153 [==============================] - 95s 5ms/step - loss: 0.2727 - val_loss: 0.2867\n",
      "Epoch 43/60\n",
      "20153/20153 [==============================] - 95s 5ms/step - loss: 0.2719 - val_loss: 0.2845\n",
      "Epoch 44/60\n",
      "20153/20153 [==============================] - 95s 5ms/step - loss: 0.2680 - val_loss: 0.2877\n",
      "Epoch 45/60\n",
      "20153/20153 [==============================] - 95s 5ms/step - loss: 0.2681 - val_loss: 0.2849\n",
      "Epoch 46/60\n",
      "20153/20153 [==============================] - 95s 5ms/step - loss: 0.2661 - val_loss: 0.2825\n",
      "Epoch 47/60\n",
      "20153/20153 [==============================] - 95s 5ms/step - loss: 0.2642 - val_loss: 0.2809\n",
      "Epoch 48/60\n",
      "20153/20153 [==============================] - 95s 5ms/step - loss: 0.2636 - val_loss: 0.2839\n",
      "Epoch 49/60\n",
      "20153/20153 [==============================] - 95s 5ms/step - loss: 0.2636 - val_loss: 0.2834\n",
      "Epoch 50/60\n",
      "20153/20153 [==============================] - 95s 5ms/step - loss: 0.2615 - val_loss: 0.2813\n",
      "Epoch 51/60\n",
      "20153/20153 [==============================] - 95s 5ms/step - loss: 0.2624 - val_loss: 0.2818\n",
      "Epoch 52/60\n",
      "20153/20153 [==============================] - 95s 5ms/step - loss: 0.2652 - val_loss: 0.2899\n",
      "Epoch 53/60\n",
      "20153/20153 [==============================] - 95s 5ms/step - loss: 0.2655 - val_loss: 0.2852\n",
      "Epoch 54/60\n",
      "20153/20153 [==============================] - 95s 5ms/step - loss: 0.2618 - val_loss: 0.2819\n",
      "Epoch 55/60\n",
      "20153/20153 [==============================] - 95s 5ms/step - loss: 0.2588 - val_loss: 0.2797\n",
      "Epoch 56/60\n",
      "20153/20153 [==============================] - 95s 5ms/step - loss: 0.2568 - val_loss: 0.2817\n",
      "Epoch 57/60\n",
      "20153/20153 [==============================] - 95s 5ms/step - loss: 0.2567 - val_loss: 0.2818\n",
      "Epoch 58/60\n",
      "20153/20153 [==============================] - 95s 5ms/step - loss: 0.2559 - val_loss: 0.2803\n",
      "Epoch 59/60\n",
      "20153/20153 [==============================] - 95s 5ms/step - loss: 0.2555 - val_loss: 0.2808\n",
      "Epoch 60/60\n",
      "20153/20153 [==============================] - 95s 5ms/step - loss: 0.2560 - val_loss: 0.2796\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2000)              4098000   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1750)              3501750   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1500)              2626500   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1250)              1876250   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1000)              1251000   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 750)               750750    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 500)               375500    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 250)               125250    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 500)               125500    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 750)               375750    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1000)              751000    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1250)              1251250   \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1500)              1876500   \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1750)              2626750   \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2000)              3502000   \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 2048)              4098048   \n",
      "=================================================================\n",
      "Total params: 29,211,798\n",
      "Trainable params: 29,211,798\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2000)              4098000   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1750)              3501750   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1500)              2626500   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1250)              1876250   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1000)              1251000   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 750)               750750    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 500)               375500    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 250)               125250    \n",
      "=================================================================\n",
      "Total params: 14,605,000\n",
      "Trainable params: 14,605,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define models\n",
    "autoencoder = Model(inputs=layer0, outputs=decoding_layer8)\n",
    "encoder = Model(inputs=layer0, outputs=encoding_layer8)\n",
    "\n",
    "# Compile the Model\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "autoencoder.fit(X_train, X_train, batch_size=64, epochs=60, validation_split=0.1, shuffle=False)\n",
    "\n",
    "# Save the model\n",
    "encoder.save('encoder.h5')\n",
    "\n",
    "autoencoder.summary()\n",
    "\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the encoder part to reduce dimensionality of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiang/.local/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "encoder = load_model('encoder.h5')\n",
    "\n",
    "# Reduce the dimensionality by passing all the features to the encoder part\n",
    "reduced_X_train = encoder.predict(X_train)\n",
    "reduced_X_test = encoder.predict(X_test)\n",
    "\n",
    "# Sanity Check: Make sure the dimensionality of the reduced features is 250\n",
    "assert reduced_X_train.shape[0] == X_train.shape[0]\n",
    "assert reduced_X_train.shape[1] == encoding_dim\n",
    "assert reduced_X_test.shape[0] == X_test.shape[0]\n",
    "assert reduced_X_test.shape[1] == encoding_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try training a linear SVM with the new features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.803470\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(kernel='linear', C=0.001)\n",
    "clf.fit(reduced_X_train, Y_train)\n",
    "accuracy = clf.score(reduced_X_test, Y_test)\n",
    "\n",
    "print('Accuracy: %f' % (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
