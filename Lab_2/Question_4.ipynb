{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric Learning -- learning a good distance metric for KNN\n",
    "\n",
    "Notes for LMNN (Large Margin Nearest Neighbors): Weinberger, K. Q., Saul, L. K. *Distance Metric Learning for Large Margin Nearest Neighbor Classification. JMLR 2009*.\n",
    "\n",
    "Often, when we want to use kNN to classification, it is possible that different dimensions of the data have different scale and thus a traditional Euclidean distance metric is not appropriate. It is desirable to give different weights to different dimensions in determining the distance b/t two points. It would be nice if we can automatically figure that distance metric out (i.e. learn a good distance metric) from the present training data set.\n",
    "\n",
    "LMNN is an algorithm whose motivation is, given the number of nearest neighbors k, learn the Mahalanobis matrix (a general way to represent the distance measure, meaning any distance metric d(x,y) can be represented by d(x,y) = (x-y)^T M (x-y)) that maximizes the kNN accuracy.\n",
    "\n",
    "LMNN can be interpresented in two ways:\n",
    "    1. Linear transformation: LMNN can be seen as a linear transformation L on the original data points X. Then it uses a simple Euclidean measure to perform a kNN classification on the transformed data points LX.\n",
    "    2. Learn a Mahalanobis distance: LMNN can also be seen as a procedure to learn a Mahalanobis distance M so that we compute the distance b/t any two points by d(x,y) = (x-y)^T M (x-y). Then we do the a kNN classification with this new distance metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from pylmnn import LargeMarginNearestNeighbor as LMNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first load the data and split it into training set (60%) and test set (40%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_file = 'AwA2-features/Animals_with_Attributes2/Features/ResNet101/AwA2-features.txt'\n",
    "labels_file = 'AwA2-features/Animals_with_Attributes2/Features/ResNet101/AwA2-labels.txt'\n",
    "\n",
    "# There is in total 37322 images of 50 classes. Each image is represented as a 2048 dimensional feature\n",
    "features = np.loadtxt(features_file) # shape (37322, 2048)\n",
    "labels = np.loadtxt(labels_file) # shape (37322, )\n",
    "\n",
    "# Split each and all classes into training set (60%) and test set (40%)\n",
    "# set random_state to an int for reproducibility\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    features, labels, train_size=0.6, test_size=0.4, random_state=0, stratify=labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a LMNN and use the trained metric to transform the data, and then build a kNN classifier out of the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 iterations of LMNN training is finished!\n",
      "Accuracy (with learned distance metric) = 0.9314756514167057\n"
     ]
    }
   ],
   "source": [
    "# Set up the hyperparameters\n",
    "k_train, k_test, n_components, max_iter = 7, 7, X_train.shape[1], 400\n",
    "\n",
    "# Instantiate the metric learner\n",
    "lmnn = LMNN(n_neighbors=k_train, max_iter=max_iter, n_components=n_components)\n",
    "\n",
    "# Train the metric learner\n",
    "lmnn.fit(X_train, Y_train)\n",
    "\n",
    "print(str(max_iter) + ' iterations of LMNN training is finished!')\n",
    "\n",
    "# Fit the nearest neighbors classifier\n",
    "clf = KNeighborsClassifier(n_neighbors=k_test)\n",
    "clf.fit(lmnn.transform(X_train), Y_train)\n",
    "\n",
    "# Compute the k-nearest neighbor test accuracy after applying the learned transformation\n",
    "accuracy = clf.score(lmnn.transform(X_test), Y_test)\n",
    "print('Accuracy (with learned distance metric) = ' + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 iterations -->  0.9082322995512091\n",
    "\n",
    "50 iterations --> 0.9210931743586308\n",
    "\n",
    "100 iterations --> 0.9182798579945073\n",
    "\n",
    "200 iteartions --> 0.9311407328019291\n",
    "\n",
    "400 iterations --> 0.9314756514167057\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
